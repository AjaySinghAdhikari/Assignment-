{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "10d143d8-8744-4c28-bf26-9d3e2769fdf0",
      "cell_type": "code",
      "source": "### Import Necessary Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "id": "182e8fed-6854-4215-81b1-93e19136b55a",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e2bd964a-e064-4938-ad79-0c497672d1c9",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "12e13c8d-19d9-4e59-a77e-f3e130204736",
      "cell_type": "code",
      "source": "# Q2: Assumptions of Linear Regression\n\"\"\"\n1. Linearity: Relationship between independent and dependent variables should be linear.\n2. Independence: Observations should be independent.\n3. Homoscedasticity: Constant variance of residuals.\n4. Normality: Residuals should be normally distributed.\n5. No multicollinearity: Independent variables should not be highly correlated.\n\nChecking assumptions:\n- Linearity: Scatter plot of actual vs. predicted values.\n- Homoscedasticity: Residual plot.\n- Normality: Histogram or Q-Q plot of residuals.\n- Multicollinearity: Variance Inflation Factor (VIF).\n\"\"\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\n1. Linearity: Relationship between independent and dependent variables should be linear.\\n2. Independence: Observations should be independent.\\n3. Homoscedasticity: Constant variance of residuals.\\n4. Normality: Residuals should be normally distributed.\\n5. No multicollinearity: Independent variables should not be highly correlated.\\n\\nChecking assumptions:\\n- Linearity: Scatter plot of actual vs. predicted values.\\n- Homoscedasticity: Residual plot.\\n- Normality: Histogram or Q-Q plot of residuals.\\n- Multicollinearity: Variance Inflation Factor (VIF).\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10
    },
    {
      "id": "b754ac19-5964-4987-accb-423621ea233c",
      "cell_type": "code",
      "source": "# Q1: Difference between Simple and Multiple Linear Regression\n\"\"\"\n- Simple Linear Regression: Models the relationship between a single independent variable (X) and a dependent variable (Y).\n    Example: Predicting house price based on square footage.\n- Multiple Linear Regression: Uses multiple independent variables to predict a dependent variable.\n    Example: Predicting house price based on square footage, number of bedrooms, and location.\n\"\"\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\n- Simple Linear Regression: Models the relationship between a single independent variable (X) and a dependent variable (Y).\\n    Example: Predicting house price based on square footage.\\n- Multiple Linear Regression: Uses multiple independent variables to predict a dependent variable.\\n    Example: Predicting house price based on square footage, number of bedrooms, and location.\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9
    },
    {
      "id": "7a8f7bfd-c871-4a3c-8b01-cad7d680e598",
      "cell_type": "code",
      "source": "# Q3: Interpreting Slope and Intercept\n\"\"\"\n- Intercept: The expected value of Y when X = 0.\n- Slope: The change in Y for a one-unit change in X.\nExample: If the slope is 50, it means every additional square foot increases house price by $50.\n\"\"\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\n- Intercept: The expected value of Y when X = 0.\\n- Slope: The change in Y for a one-unit change in X.\\nExample: If the slope is 50, it means every additional square foot increases house price by $50.\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11
    },
    {
      "id": "08ef31cf-a2ab-434e-b33f-88603ec23e0c",
      "cell_type": "code",
      "source": "# Q4: Gradient Descent\n\"\"\"\nGradient Descent is an optimization algorithm used to minimize the cost function by iteratively updating model parameters.\nFormula: θ = θ - α * (dJ/dθ)\nwhere α is the learning rate.\nUsed in machine learning to optimize models like linear regression and neural networks.\n\"\"\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\nGradient Descent is an optimization algorithm used to minimize the cost function by iteratively updating model parameters.\\nFormula: θ = θ - α * (dJ/dθ)\\nwhere α is the learning rate.\\nUsed in machine learning to optimize models like linear regression and neural networks.\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12
    },
    {
      "id": "9eff6609-9b09-4c6b-a9a4-bc5c1c0af692",
      "cell_type": "code",
      "source": "# Q5: Multiple Linear Regression Model\n\"\"\"\nY = β0 + β1X1 + β2X2 + ... + βnXn + ε\nIt differs from simple linear regression as it includes multiple predictors.\n\"\"\"\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\nY = β0 + β1X1 + β2X2 + ... + βnXn + ε\\nIt differs from simple linear regression as it includes multiple predictors.\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13
    },
    {
      "id": "4e85ec66-eb1b-4f73-8b51-121e95c7a0a5",
      "cell_type": "code",
      "source": "\n# Q6: Multicollinearity\n\"\"\"\nOccurs when independent variables are highly correlated, affecting coefficient estimates.\nDetection:\n- Correlation matrix\n- Variance Inflation Factor (VIF) (VIF > 10 indicates strong multicollinearity)\nSolution:\n- Remove highly correlated variables\n- Use Ridge Regression\n- Principal Component Analysis (PCA)\n\"\"\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\nOccurs when independent variables are highly correlated, affecting coefficient estimates.\\nDetection:\\n- Correlation matrix\\n- Variance Inflation Factor (VIF) (VIF > 10 indicates strong multicollinearity)\\nSolution:\\n- Remove highly correlated variables\\n- Use Ridge Regression\\n- Principal Component Analysis (PCA)\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14
    },
    {
      "id": "43d408ac-fde8-464c-95e9-0efef413668a",
      "cell_type": "code",
      "source": "# Q7: Polynomial Regression\n\"\"\"\nPolynomial Regression models non-linear relationships by adding polynomial terms of the independent variable.\nFormula: Y = β0 + β1X + β2X^2 + ... + βnX^n + ε\n\"\"\"\n\n# Example Implementation\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X_simple)\nmodel_poly = LinearRegression().fit(X_poly, y_simple)\nprint(\"Polynomial Regression Coefficients:\", model_poly.coef_)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'X_simple' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Example Implementation\u001b[39;00m\n\u001b[1;32m      8\u001b[0m poly \u001b[38;5;241m=\u001b[39m PolynomialFeatures(degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m X_poly \u001b[38;5;241m=\u001b[39m poly\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_simple\u001b[49m)\n\u001b[1;32m     10\u001b[0m model_poly \u001b[38;5;241m=\u001b[39m LinearRegression()\u001b[38;5;241m.\u001b[39mfit(X_poly, y_simple)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolynomial Regression Coefficients:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_poly\u001b[38;5;241m.\u001b[39mcoef_)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_simple' is not defined"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 15
    },
    {
      "id": "c237431f-6927-4a80-b103-d64133df344c",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}